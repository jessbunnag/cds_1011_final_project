{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from encoder import *\n",
    "from AttnDecoder import * \n",
    "from seq2seq import *\n",
    "\n",
    "from build_dataset import *\n",
    "from inference import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example:\n",
    "hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "               'ensures', 'that', 'the', 'military', 'always',\n",
    "               'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "hypothesis2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was', 'interested', 'in', 'world', 'history']\n",
    "hypotheses = [hypothesis1, hypothesis2]               \n",
    "\n",
    "reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "               'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
    "               'heed', 'Party', 'commands']                  \n",
    "reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "               'guarantees', 'the', 'military', 'forces', 'always',\n",
    "               'being', 'under', 'the', 'command', 'of', 'the',\n",
    "               'Party']               \n",
    "reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "               'of', 'the', 'party']              \n",
    "ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because', 'he', 'read', 'the', 'book']                \n",
    "references = [[reference1, reference2, reference3], [ref2a]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41180376356915777"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([reference1], hypothesis1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence'], ['This', 'is', 'another', 'sentence']]\n",
    "labels_list = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']], [['Another', 'Different', 'One']] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 1/3 1/3 1/3\n"
     ]
    }
   ],
   "source": [
    "# NLTK - modified_precision\n",
    "# compute the BLEU score across all sentences for n in [1, 4]\n",
    "all_bleu_1 = []\n",
    "all_bleu_2 = []\n",
    "all_bleu_3 = []\n",
    "all_bleu_4 = []\n",
    "\n",
    "for idx in range(len(labels_list)):\n",
    "    all_bleu_1.append(modified_precision(labels_list[idx], preds_list[idx], n=(idx+1)))\n",
    "    all_bleu_2.append(modified_precision(labels_list[idx], preds_list[idx], n=(idx+2)))\n",
    "    all_bleu_3.append(modified_precision(labels_list[idx], preds_list[idx], n=(idx+3)))\n",
    "    all_bleu_4.append(modified_precision(labels_list[idx], preds_list[idx], n=(idx+4)))\n",
    "\n",
    "print(np.mean(all_bleu_1), np.mean(all_bleu_2), np.mean(all_bleu_3), np.mean(all_bleu_4))\n",
    "# print(round(np.mean(all_bleu_1),4), round(np.mean(all_bleu_2),4), round(np.mean(all_bleu_3),4), round(np.mean(all_bleu_4),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9222222222222223 0.6274509803921569 0.46875 0.2761904761904762\n"
     ]
    }
   ],
   "source": [
    "# NLTK - modified_precision\n",
    "# compute the BLEU score across all sentences for n in [1, 4]\n",
    "all_bleu_1 = []\n",
    "all_bleu_2 = []\n",
    "all_bleu_3 = []\n",
    "all_bleu_4 = []\n",
    "\n",
    "for idx in range(len(references)):\n",
    "    all_bleu_1.append(modified_precision(references[idx], hypotheses[idx], n=(idx+1)))\n",
    "    all_bleu_2.append(modified_precision(references[idx], hypotheses[idx], n=(idx+2)))\n",
    "    all_bleu_3.append(modified_precision(references[idx], hypotheses[idx], n=(idx+3)))\n",
    "    all_bleu_4.append(modified_precision(references[idx], hypotheses[idx], n=(idx+4)))\n",
    "\n",
    "print(float(np.mean(all_bleu_1)), float(np.mean(all_bleu_2)), float(np.mean(all_bleu_3)), float(np.mean(all_bleu_4)))\n",
    "# print(round(np.mean(all_bleu_1),4), round(np.mean(all_bleu_2),4), round(np.mean(all_bleu_3),4), round(np.mean(all_bleu_4),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722222222222222 0.8470196452752219 0.7337526821183125 0.6223247442490669\n"
     ]
    }
   ],
   "source": [
    "# NLTK - sentence_bleu:\n",
    "# compute the BLEU score across all sentences for n in [1, 4]\n",
    "all_bleu_1 = []\n",
    "all_bleu_2 = []\n",
    "all_bleu_3 = []\n",
    "all_bleu_4 = []\n",
    "\n",
    "weights1 = [1.0/1.0]\n",
    "weights2 = [1.0/2.0, 1.0/2.0]\n",
    "weights3 = [1.0/3.0, 1.0/3.0, 1.0/3.0]\n",
    "weights4 = [1.0/4.0, 1.0/4.0, 1.0/4.0, 1.0/4.0]\n",
    "\n",
    "for idx in range(len(references)):\n",
    "    all_bleu_1.append(sentence_bleu(references[idx], hypotheses[idx], weights1))\n",
    "    all_bleu_2.append(sentence_bleu(references[idx], hypotheses[idx], weights2))\n",
    "    all_bleu_3.append(sentence_bleu(references[idx], hypotheses[idx], weights3))\n",
    "    all_bleu_4.append(sentence_bleu(references[idx], hypotheses[idx], weights4))\n",
    "\n",
    "print(np.mean(all_bleu_1), np.mean(all_bleu_2), np.mean(all_bleu_3), np.mean(all_bleu_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BLEUScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/soominkim/NYU/F22/1011-NLP/cds_1011_final_project/train-testing.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/soominkim/NYU/F22/1011-NLP/cds_1011_final_project/train-testing.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m all_bleu_3 \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/soominkim/NYU/F22/1011-NLP/cds_1011_final_project/train-testing.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m all_bleu_4 \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/soominkim/NYU/F22/1011-NLP/cds_1011_final_project/train-testing.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m metric1 \u001b[39m=\u001b[39m BLEUScore(n_gram\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/soominkim/NYU/F22/1011-NLP/cds_1011_final_project/train-testing.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m metric2 \u001b[39m=\u001b[39m BLEUScore(n_gram\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/soominkim/NYU/F22/1011-NLP/cds_1011_final_project/train-testing.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m metric3 \u001b[39m=\u001b[39m BLEUScore(n_gram\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BLEUScore' is not defined"
     ]
    }
   ],
   "source": [
    "# Torchmetrics - BLEUScore\n",
    "# compute the BLEU score across all sentences for n in [1, 4]\n",
    "all_bleu_1 = []\n",
    "all_bleu_2 = []\n",
    "all_bleu_3 = []\n",
    "all_bleu_4 = []\n",
    "\n",
    "metric1 = BLEUScore(n_gram=1)\n",
    "metric2 = BLEUScore(n_gram=2)\n",
    "metric3 = BLEUScore(n_gram=3)\n",
    "metric4 = BLEUScore(n_gram=4)\n",
    "for idx in range(len(labels_list)):\n",
    "    all_bleu_1.append(metric1(preds_list[idx], labels_list[idx]))\n",
    "    all_bleu_2.append(metric2(preds_list[idx], labels_list[idx]))\n",
    "    all_bleu_3.append(metric3(preds_list[idx], labels_list[idx]))\n",
    "    all_bleu_4.append(metric4(preds_list[idx], labels_list[idx]))\n",
    "\n",
    "print(np.mean(all_bleu_1), np.mean(all_bleu_2), np.mean(all_bleu_3), np.mean(all_bleu_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/soomink/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/share/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/soomink/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/share/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m all_meteor \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(labels_list)):\n\u001b[0;32m----> 5\u001b[0m     all_meteor\u001b[39m.\u001b[39mappend(meteor_score(labels_list[idx], preds_list[idx]))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/translate/meteor_score.py:397\u001b[0m, in \u001b[0;36mmeteor_score\u001b[0;34m(references, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeteor_score\u001b[39m(\n\u001b[1;32m    348\u001b[0m     references: Iterable[Iterable[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m    349\u001b[0m     hypothesis: Iterable[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m     gamma: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m,\n\u001b[1;32m    356\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    357\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m    Calculates METEOR score for hypothesis with multiple references as\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m    described in \"Meteor: An Automatic Metric for MT Evaluation with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39;49m(\n\u001b[1;32m    398\u001b[0m         single_meteor_score(\n\u001b[1;32m    399\u001b[0m             reference,\n\u001b[1;32m    400\u001b[0m             hypothesis,\n\u001b[1;32m    401\u001b[0m             preprocess\u001b[39m=\u001b[39;49mpreprocess,\n\u001b[1;32m    402\u001b[0m             stemmer\u001b[39m=\u001b[39;49mstemmer,\n\u001b[1;32m    403\u001b[0m             wordnet\u001b[39m=\u001b[39;49mwordnet,\n\u001b[1;32m    404\u001b[0m             alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    405\u001b[0m             beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m    406\u001b[0m             gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    407\u001b[0m         )\n\u001b[1;32m    408\u001b[0m         \u001b[39mfor\u001b[39;49;00m reference \u001b[39min\u001b[39;49;00m references\n\u001b[1;32m    409\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/translate/meteor_score.py:398\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeteor_score\u001b[39m(\n\u001b[1;32m    348\u001b[0m     references: Iterable[Iterable[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m    349\u001b[0m     hypothesis: Iterable[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m     gamma: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m,\n\u001b[1;32m    356\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    357\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m    Calculates METEOR score for hypothesis with multiple references as\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m    described in \"Meteor: An Automatic Metric for MT Evaluation with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39m    :return: The sentence-level METEOR score.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\n\u001b[0;32m--> 398\u001b[0m         single_meteor_score(\n\u001b[1;32m    399\u001b[0m             reference,\n\u001b[1;32m    400\u001b[0m             hypothesis,\n\u001b[1;32m    401\u001b[0m             preprocess\u001b[39m=\u001b[39;49mpreprocess,\n\u001b[1;32m    402\u001b[0m             stemmer\u001b[39m=\u001b[39;49mstemmer,\n\u001b[1;32m    403\u001b[0m             wordnet\u001b[39m=\u001b[39;49mwordnet,\n\u001b[1;32m    404\u001b[0m             alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    405\u001b[0m             beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m    406\u001b[0m             gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    407\u001b[0m         )\n\u001b[1;32m    408\u001b[0m         \u001b[39mfor\u001b[39;00m reference \u001b[39min\u001b[39;00m references\n\u001b[1;32m    409\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/translate/meteor_score.py:331\u001b[0m, in \u001b[0;36msingle_meteor_score\u001b[0;34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    329\u001b[0m translation_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(enum_hypothesis)\n\u001b[1;32m    330\u001b[0m reference_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(enum_reference)\n\u001b[0;32m--> 331\u001b[0m matches, _, _ \u001b[39m=\u001b[39m _enum_align_words(\n\u001b[1;32m    332\u001b[0m     enum_hypothesis, enum_reference, stemmer\u001b[39m=\u001b[39;49mstemmer, wordnet\u001b[39m=\u001b[39;49mwordnet\n\u001b[1;32m    333\u001b[0m )\n\u001b[1;32m    334\u001b[0m matches_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(matches)\n\u001b[1;32m    335\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/translate/meteor_score.py:223\u001b[0m, in \u001b[0;36m_enum_align_words\u001b[0;34m(enum_hypothesis_list, enum_reference_list, stemmer, wordnet)\u001b[0m\n\u001b[1;32m    215\u001b[0m exact_matches, enum_hypothesis_list, enum_reference_list \u001b[39m=\u001b[39m _match_enums(\n\u001b[1;32m    216\u001b[0m     enum_hypothesis_list, enum_reference_list\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    219\u001b[0m stem_matches, enum_hypothesis_list, enum_reference_list \u001b[39m=\u001b[39m _enum_stem_match(\n\u001b[1;32m    220\u001b[0m     enum_hypothesis_list, enum_reference_list, stemmer\u001b[39m=\u001b[39mstemmer\n\u001b[1;32m    221\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m wns_matches, enum_hypothesis_list, enum_reference_list \u001b[39m=\u001b[39m _enum_wordnetsyn_match(\n\u001b[1;32m    224\u001b[0m     enum_hypothesis_list, enum_reference_list, wordnet\u001b[39m=\u001b[39;49mwordnet\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     \u001b[39msorted\u001b[39m(\n\u001b[1;32m    229\u001b[0m         exact_matches \u001b[39m+\u001b[39m stem_matches \u001b[39m+\u001b[39m wns_matches, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m wordpair: wordpair[\u001b[39m0\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     enum_reference_list,\n\u001b[1;32m    233\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/translate/meteor_score.py:161\u001b[0m, in \u001b[0;36m_enum_wordnetsyn_match\u001b[0;34m(enum_hypothesis_list, enum_reference_list, wordnet)\u001b[0m\n\u001b[1;32m    152\u001b[0m word_match \u001b[39m=\u001b[39m []\n\u001b[1;32m    153\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(enum_hypothesis_list))[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    154\u001b[0m     hypothesis_syns \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    155\u001b[0m         chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m    156\u001b[0m             (\n\u001b[1;32m    157\u001b[0m                 lemma\u001b[39m.\u001b[39mname()\n\u001b[1;32m    158\u001b[0m                 \u001b[39mfor\u001b[39;00m lemma \u001b[39min\u001b[39;00m synset\u001b[39m.\u001b[39mlemmas()\n\u001b[1;32m    159\u001b[0m                 \u001b[39mif\u001b[39;00m lemma\u001b[39m.\u001b[39mname()\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0m             \u001b[39mfor\u001b[39;00m synset \u001b[39min\u001b[39;00m wordnet\u001b[39m.\u001b[39;49msynsets(enum_hypothesis_list[i][\u001b[39m1\u001b[39m])\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m     )\u001b[39m.\u001b[39munion({enum_hypothesis_list[i][\u001b[39m1\u001b[39m]})\n\u001b[1;32m    164\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(enum_reference_list))[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    165\u001b[0m         \u001b[39mif\u001b[39;00m enum_reference_list[j][\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m hypothesis_syns:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/soomink/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/share/nltk_data'\n    - '/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# compute the METEOR score:\n",
    "all_meteor = []\n",
    "\n",
    "for idx in range(len(labels_list)):\n",
    "    all_meteor.append(meteor_score(labels_list[idx], preds_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_fmeasure': tensor(0.7500), 'rouge1_precision': tensor(0.7500), 'rouge1_recall': tensor(0.7500), 'rouge2_fmeasure': tensor(0.), 'rouge2_precision': tensor(0.), 'rouge2_recall': tensor(0.), 'rougeL_fmeasure': tensor(0.5000), 'rougeL_precision': tensor(0.5000), 'rougeL_recall': tensor(0.5000), 'rougeLsum_fmeasure': tensor(0.5000), 'rougeLsum_precision': tensor(0.5000), 'rougeLsum_recall': tensor(0.5000)}\n"
     ]
    }
   ],
   "source": [
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "preds = \"My name is John\"\n",
    "target = \"Is your name John\"\n",
    "rouge = ROUGEScore()\n",
    "print(rouge(preds, target).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cds-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f0f61664e64e054731fd7a0d3b8fe7c49ba104fb8b8d2ab4ae527096366d455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
