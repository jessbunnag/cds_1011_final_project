{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/conda/1011_project/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/tb2817/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/tb2817/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from encoder import *\n",
    "from AttnDecoder import * \n",
    "from seq2seq_transformer import *\n",
    "\n",
    "from build_dataset import *\n",
    "from inference import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from greedy_search import generate_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set whether or not model is pretrained\n",
    "model_pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "main_data_path = \"data/processed\"\n",
    "\n",
    "train_file_path = {\n",
    "    'source': f\"{main_data_path}/src-train.txt\",\n",
    "    'target': f\"{main_data_path}/tgt-train.txt\"\n",
    "}\n",
    "\n",
    "test_file_path = {\n",
    "    'source': f\"{main_data_path}/src-test.txt\",\n",
    "    'target': f\"{main_data_path}/tgt-test.txt\"\n",
    "}\n",
    "\n",
    "dev_file_path = {\n",
    "    'source': f\"{main_data_path}/src-dev.txt\",\n",
    "    'target': f\"{main_data_path}/tgt-dev.txt\"\n",
    "}\n",
    "\n",
    "# build vocab with train data only \n",
    "vocab = build_train_vocab(train_file_path)\n",
    "\n",
    "# build datasets for all train, test, dev\n",
    "datasets = {\n",
    "    'train': QAPair(train_file_path, vocab),\n",
    "    'test': QAPair(test_file_path, vocab),\n",
    "    'dev': QAPair(dev_file_path, vocab),\n",
    "}\n",
    "\n",
    "# build dataloaders\n",
    "batch_size = 64\n",
    "dataloaders = {}\n",
    "for split, dataset in datasets.items():\n",
    "    dataloaders[split] = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=partial(pad_collate_fn, pad_token=dataset.pad_idx)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load parsed GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "pretrained_vectors = {\n",
    "    'enc': torch.load(f'embeddings/encoder_emb_{embed_size}.pt').float(),\n",
    "    'dec': torch.load(f'embeddings/decoder_emb_{embed_size}.pt').float()\n",
    "}\n",
    "\n",
    "# input_size = len(train_dataset.answer_vocab) \n",
    "output_size = len(vocab['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define train and eval steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    source = batch.source_vecs.to(device) \n",
    "    source_len = batch.source_lens.to(device)\n",
    "    target = batch.target_vecs.to(device)\n",
    "    target_len = batch.target_lens.to(device) \n",
    "\n",
    "    target_input = target[:, :-1]\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(source, target_input)\n",
    "\n",
    "    logits = model(source, target_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    target_out = target[:, 1:]\n",
    "    # print(f'dec_log_probs {dec_log_probs.shape}')\n",
    "    # print(f'target_out {target_out.shape}')\n",
    "    loss = criterion(logits.transpose(1, 2), target_out)\n",
    "    loss.backward()\n",
    "    # clip gradients\n",
    "    # nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0, norm_type=2)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # return the attention scores at the last time step\n",
    "    return loss.item(), logits\n",
    "\n",
    "def eval_step(batch, model, criterion, device):\n",
    "    model.eval()\n",
    "    source = batch.source_vecs.to(device) \n",
    "    inputs_len = batch.source_lens.to(device)\n",
    "    target = batch.target_vecs.to(device)\n",
    "    target_len = batch.target_lens.to(device) \n",
    "\n",
    "    target_input = target[:, :-1]\n",
    "\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(source, target_input)\n",
    "\n",
    "    logits = model(source, target_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "    \n",
    "    target_out = target[:, 1:]\n",
    "\n",
    "    # scores = s2s_output.view(-1, s2s_output.size(-1))\n",
    "    \n",
    "    loss = criterion(logits.transpose(1, 2), target_out)\n",
    "\n",
    "    # return the attention scores at the last time step\n",
    "    return loss.item(), logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN LOOP \n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "EMBED_SIZE = 300\n",
    "NHEAD = 10\n",
    "\n",
    "if not model_pretrained:\n",
    "    # initial learning rate\n",
    "    lr = 0.01\n",
    "    # initialize the model, optimizer, and criterion\n",
    "    seq2seq = Seq2SeqTransformer(\n",
    "        NUM_ENCODER_LAYERS,\n",
    "        NUM_DECODER_LAYERS,\n",
    "        EMBED_SIZE,\n",
    "        NHEAD,\n",
    "        pretrained_vectors, \n",
    "        tgt_vocab_size=len(vocab['target'])\n",
    "    )\n",
    "    optimizer = torch.optim.SGD(seq2seq.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=datasets['train'].pad_idx)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    seq2seq.to(device)\n",
    "\n",
    "    plot_cache = {}\n",
    "    plot_cache['train'] = []\n",
    "    plot_cache['dev'] = []\n",
    "\n",
    "    # halve lr at epoch 8\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma=0.5) \n",
    "\n",
    "    NUM_EPOCHS = 15\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):    \n",
    "        # train \n",
    "        train_losses = []\n",
    "        for i, data in tqdm(enumerate(dataloaders['train']), leave=False):\n",
    "            curr_loss, dec_log_probs = train_step(data, seq2seq, optimizer, criterion, device)\n",
    "\n",
    "            train_losses.append(curr_loss)\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        print(f'Train loss after epoch {epoch+1} = {avg_train_loss}')\n",
    "\n",
    "        # eval \n",
    "        test_losses = []\n",
    "        with torch.no_grad():\n",
    "            for i, data in tqdm(enumerate(dataloaders['dev']), leave=False):\n",
    "                curr_loss, dec_log_probs = eval_step(data, seq2seq, criterion, device)\n",
    "\n",
    "                test_losses.append(curr_loss)\n",
    "\n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "            print(f'Dev loss after epoch {epoch+1} = {avg_test_loss}')\n",
    "\n",
    "        plot_cache['train'].append(avg_train_loss)\n",
    "        plot_cache['dev'].append(avg_test_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # save the model state\n",
    "    torch.save({'state_dict': seq2seq.state_dict(), 'plot_cache': plot_cache}, 'trained_transformer_statedict.pt')\n",
    "else:\n",
    "    # reload the model\n",
    "    seq2seq = Seq2SeqTransformer(\n",
    "        NUM_ENCODER_LAYERS,\n",
    "        NUM_DECODER_LAYERS,\n",
    "        EMBED_SIZE,\n",
    "        NHEAD,\n",
    "        pretrained_vectors, \n",
    "        tgt_vocab_size=len(vocab['target'])\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=datasets['train'].pad_idx)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seq2seq.to(device)\n",
    "\n",
    "    checkpoint = torch.load('trained_transformer_statedict.pt')\n",
    "    seq2seq.load_state_dict(checkpoint['state_dict'])\n",
    "    plot_cache = checkpoint['plot_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss = 5.281587436635007\n",
      "Average BLEU_1 Score = 0.21945100910726836\n",
      "Average BLEU_2 Score = 0.048696900012918\n",
      "Average BLEU_3 Score = 0.011120281844419898\n",
      "Average BLEU_4 Score = 0.006482438410266793\n",
      "Average METEOR Score = 0.12119096413970686\n",
      "Average ROUGE_L Score = 0.1724618822336197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# run inference on the test set\n",
    "test_losses = []\n",
    "all_best_attn_labels = []\n",
    "all_preds_list = []\n",
    "all_labels_list = []\n",
    "raw_labels_list = []\n",
    "raw_source_list = []\n",
    "all_cleaned_preds_list = []\n",
    "all_bleu_1 = []\n",
    "all_bleu_2 = []\n",
    "all_bleu_3 = []\n",
    "all_bleu_4 = []\n",
    "all_meteor = []\n",
    "all_rouge_l = []\n",
    "with torch.no_grad():\n",
    "    for i, data in tqdm(enumerate(dataloaders['test']), leave=False):\n",
    "        # run evaluation on the test set\n",
    "        curr_loss, dec_log_probs = eval_step(data, seq2seq, criterion, device)\n",
    "        test_losses.append(curr_loss)\n",
    "\n",
    "        # run vanilla inference on the test set\n",
    "        preds_list, labels_list = vanilla_inference(dec_log_probs, data.target_vecs, vocab['target'])\n",
    "\n",
    "        # evaluate the predictions with the metrics\n",
    "        bleu_1, bleu_2, bleu_3, bleu_4, meteor, rouge_l = eval_metrics(preds_list, labels_list) # meteor, rouge_l\n",
    "        all_bleu_1.append(bleu_1)\n",
    "        all_bleu_2.append(bleu_2)\n",
    "        all_bleu_3.append(bleu_3)\n",
    "        all_bleu_4.append(bleu_4)\n",
    "        all_meteor.append(meteor)\n",
    "        all_rouge_l.append(rouge_l)\n",
    "\n",
    "        all_preds_list.append(preds_list)\n",
    "        all_labels_list.append(labels_list)\n",
    "        # all_cleaned_preds_list.append(cleaned_preds_list)\n",
    "        raw_labels_list.append(data.target_data)\n",
    "        raw_source_list.append(data.source_data)\n",
    "\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    avg_bleu_1 = np.mean(all_bleu_1)\n",
    "    avg_bleu_2 = np.mean(all_bleu_2)\n",
    "    avg_bleu_3 = np.mean(all_bleu_3)\n",
    "    avg_bleu_4 = np.mean(all_bleu_4)\n",
    "    avg_meteor = np.mean(all_meteor)\n",
    "    avg_rouge_l = np.mean(all_rouge_l)\n",
    "\n",
    "    print(f'Average Test Loss = {avg_test_loss}')\n",
    "    print(f'Average BLEU_1 Score = {avg_bleu_1}')\n",
    "    print(f'Average BLEU_2 Score = {avg_bleu_2}')\n",
    "    print(f'Average BLEU_3 Score = {avg_bleu_3}')\n",
    "    print(f'Average BLEU_4 Score = {avg_bleu_4}')\n",
    "    print(f'Average METEOR Score = {avg_meteor}')\n",
    "    print(f'Average ROUGE_L Score = {avg_rouge_l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src torch.Size([26])\n",
      "src_mask torch.Size([26, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ys torch.Size([1, 1])\n",
      "memory torch.Size([26, 26, 300])\n",
      "tgt_mask torch.Size([1, 1])\n",
      "embedding torch.Size([1, 1, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[26, 10, 30]' is invalid for input of size 202800",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/scratch/tb2817/1011_project/train_transformer.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgreenecomputecontainer/scratch/tb2817/1011_project/train_transformer.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m labels_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgreenecomputecontainer/scratch/tb2817/1011_project/train_transformer.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39msource_vecs)):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgreenecomputecontainer/scratch/tb2817/1011_project/train_transformer.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     pred \u001b[39m=\u001b[39m generate_question(seq2seq, data\u001b[39m.\u001b[39;49msource_data[j], vocab)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgreenecomputecontainer/scratch/tb2817/1011_project/train_transformer.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     preds_list\u001b[39m.\u001b[39mappend(pred)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgreenecomputecontainer/scratch/tb2817/1011_project/train_transformer.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     labels_list\u001b[39m.\u001b[39mappend(data\u001b[39m.\u001b[39mtarget_data[i])\n",
      "File \u001b[0;32m/scratch/tb2817/1011_project/greedy_search.py:44\u001b[0m, in \u001b[0;36mgenerate_question\u001b[0;34m(model, src_sentence, vocab)\u001b[0m\n\u001b[1;32m     42\u001b[0m num_tokens \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m src_mask \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mzeros(num_tokens, num_tokens))\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mbool)\n\u001b[0;32m---> 44\u001b[0m tgt_tokens \u001b[39m=\u001b[39m greedy_decode(\n\u001b[1;32m     45\u001b[0m     model,  src, src_mask, max_len\u001b[39m=\u001b[39;49mnum_tokens \u001b[39m+\u001b[39;49m \u001b[39m5\u001b[39;49m, start_symbol\u001b[39m=\u001b[39;49mBOS_IDX)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m vocab[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdecode_idx2token(tgt_tokens\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/scratch/tb2817/1011_project/greedy_search.py:25\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, src, src_mask, max_len, start_symbol)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmemory \u001b[39m\u001b[39m{\u001b[39;00mmemory\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtgt_mask \u001b[39m\u001b[39m{\u001b[39;00mtgt_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(ys, memory, tgt_mask)\n\u001b[1;32m     26\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerator(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/scratch/tb2817/1011_project/seq2seq_transformer.py:85\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.decode\u001b[0;34m(self, tgt, memory, tgt_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(\n\u001b[1;32m     83\u001b[0m                   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_tok_emb(tgt))\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39membedding \u001b[39m\u001b[39m{\u001b[39;00membedding\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mdecoder(embedding, memory,\n\u001b[1;32m     86\u001b[0m                   tgt_mask)\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/transformer.py:293\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    290\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[1;32m    292\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 293\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[1;32m    294\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[1;32m    295\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[1;32m    296\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/transformer.py:579\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask))\n\u001b[0;32m--> 579\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mha_block(x, memory, memory_mask, memory_key_padding_mask))\n\u001b[1;32m    580\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/transformer.py:596\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mha_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    595\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 596\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(x, mem, mem,\n\u001b[1;32m    597\u001b[0m                             attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    598\u001b[0m                             key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    599\u001b[0m                             need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    600\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/modules/activation.py:1155\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1145\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1146\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1153\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1155\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1156\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1157\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1159\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1160\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1161\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1162\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1164\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/ext3/conda/1011_project/lib/python3.8/site-packages/torch/nn/functional.py:5128\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5126\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz \u001b[39m*\u001b[39m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5127\u001b[0m \u001b[39mif\u001b[39;00m static_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 5128\u001b[0m     k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mview(k\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], bsz \u001b[39m*\u001b[39;49m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   5129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5130\u001b[0m     \u001b[39m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5131\u001b[0m     \u001b[39massert\u001b[39;00m static_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m bsz \u001b[39m*\u001b[39m num_heads, \\\n\u001b[1;32m   5132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting static_k.size(0) of \u001b[39m\u001b[39m{\u001b[39;00mbsz \u001b[39m*\u001b[39m num_heads\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mstatic_k\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[26, 10, 30]' is invalid for input of size 202800"
     ]
    }
   ],
   "source": [
    "# run inference on the test set\n",
    "test_losses = []\n",
    "all_best_attn_labels = []\n",
    "all_preds_list = []\n",
    "all_labels_list = []\n",
    "raw_labels_list = []\n",
    "raw_source_list = []\n",
    "all_cleaned_preds_list = []\n",
    "all_bleu_1 = []\n",
    "all_bleu_2 = []\n",
    "all_bleu_3 = []\n",
    "all_bleu_4 = []\n",
    "all_meteor = []\n",
    "all_rouge_l = []\n",
    "with torch.no_grad():\n",
    "    for i, data in tqdm(enumerate(dataloaders['test']), leave=False):\n",
    "        # run evaluation on the test set\n",
    "        preds_list = []\n",
    "        labels_list = []\n",
    "        for j in range(len(data.source_vecs)):\n",
    "            pred = generate_question(seq2seq, data.source_data[j], vocab)\n",
    "            preds_list.append(pred)\n",
    "            labels_list.append(data.target_data[i])\n",
    "\n",
    "        # evaluate the predictions with the metrics\n",
    "        bleu_1, bleu_2, bleu_3, bleu_4, meteor, rouge_l = eval_metrics(preds_list, labels_list) # meteor, rouge_l\n",
    "        all_bleu_1.append(bleu_1)\n",
    "        all_bleu_2.append(bleu_2)\n",
    "        all_bleu_3.append(bleu_3)\n",
    "        all_bleu_4.append(bleu_4)\n",
    "        all_meteor.append(meteor)\n",
    "        all_rouge_l.append(rouge_l)\n",
    "\n",
    "        all_preds_list.append(preds_list)\n",
    "        all_labels_list.append(labels_list)\n",
    "        # all_cleaned_preds_list.append(cleaned_preds_list)\n",
    "        raw_labels_list.append(data.target_data)\n",
    "        raw_source_list.append(data.source_data)\n",
    "\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    avg_bleu_1 = np.mean(all_bleu_1)\n",
    "    avg_bleu_2 = np.mean(all_bleu_2)\n",
    "    avg_bleu_3 = np.mean(all_bleu_3)\n",
    "    avg_bleu_4 = np.mean(all_bleu_4)\n",
    "    # avg_meteor = np.mean(all_meteor)\n",
    "    # avg_rouge_l = np.mean(all_rouge_l)\n",
    "\n",
    "    print(f'Average Test Loss = {avg_test_loss}')\n",
    "    print(f'Average BLEU_1 Score = {avg_bleu_1}')\n",
    "    print(f'Average BLEU_2 Score = {avg_bleu_2}')\n",
    "    print(f'Average BLEU_3 Score = {avg_bleu_3}')\n",
    "    print(f'Average BLEU_4 Score = {avg_bleu_4}')\n",
    "    # print(f'Average METEOR Score = {avg_meteor}')\n",
    "    # print(f'Average ROUGE_L Score = {avg_rouge_l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (Answer) Sentence:\t in europe during the medieval period , guilds were formed by craftsmen to organise their trades and written contracts have survived , particularly in relation to ecclesiastical buildings .\n",
      "Target (Question) Label:\t what organizations were created by medieval craftsmen ?\n",
      "Target (Question) Raw Prediction:\t was did the the the ? ? <eos>\n",
      "\n",
      "Source (Answer) Sentence:\t a total of 120 torchbearers were selected to participate in the event consisting of celebrities , athletes and pro-beijing camp politicians .\n",
      "Target (Question) Label:\t the torchbearers included athletes , celebrities and who ?\n",
      "Target (Question) Raw Prediction:\t name of of of what what what was <eos>\n",
      "\n",
      "Source (Answer) Sentence:\t the collateralized debt obligation in particular enabled financial institutions to obtain investor funds to finance subprime and other lending , extending or increasing the housing bubble and generating large fees .\n",
      "Target (Question) Label:\t what is the name of the securities that enabled financial institutions to obtain investor funds to finance subprime ?\n",
      "Target (Question) Raw Prediction:\t is the name of the name of the of of of the to ? ? the ? ? <eos>\n",
      "\n",
      "Source (Answer) Sentence:\t in the first quarter of 2014 , the average weekly wage in new york county -lrb- manhattan -rrb- was $ 2,749 , representing the highest total among large counties in the united states .\n",
      "Target (Question) Label:\t what is the average weekly wage in manhattan ?\n",
      "Target (Question) Raw Prediction:\t was the name of of of the in <eos>\n",
      "\n",
      "Source (Answer) Sentence:\t in climates with significant heating loads , deciduous trees should not be planted on the equator facing side of a building because they will interfere with winter solar availability .\n",
      "Target (Question) Label:\t why should trees not be planted on the side of a building facing the equator ?\n",
      "Target (Question) Raw Prediction:\t is the is the the ? the <unk> of the the of ? <unk> ? <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out a few postprocesse predictions\n",
    "for i in range(5):\n",
    "    print('Source (Answer) Sentence:\\t', \" \".join(raw_source_list[0][i]))\n",
    "    print('Target (Question) Label:\\t', \" \".join(raw_labels_list[0][i]))\n",
    "    print('Target (Question) Raw Prediction:\\t', \" \".join(all_preds_list[0][i]))\n",
    "    # print('Target (Question) Postprocessed Prediction:', \" \".join(all_cleaned_preds_list[0][i]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAIN AND VAL LOSS\n",
    "import matplotlib.pyplot as plt \n",
    "def plot_over_training(per_epoch_metrics, title_name, num_epochs):\n",
    "    t = np.arange(1, num_epochs+1)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    colors = ['tab:blue', 'tab:red']\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('loss')\n",
    "\n",
    "    for key, color in zip(per_epoch_metrics, colors):\n",
    "        label = f'{key}_loss'\n",
    "        ax1.plot(t, per_epoch_metrics[key], color=color, linewidth=1, label=label)\n",
    "\n",
    "    ax1.tick_params(axis='y')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout\n",
    "    plt.title(title_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_over_training(plot_cache, 'Train and val loss over 15 epochs', 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d10e99a7d3ce7799ff53fffb01ded82fc9320fdeabd270dd8a29b265097acaf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
