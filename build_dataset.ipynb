{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/conda/1011_project/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import time \n",
    "import json \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, lil_tokens, max_vocab_size=10_000):\n",
    "        self.lil_tokens = lil_tokens\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "\n",
    "        # add special tokens \n",
    "        self.tokens.append('<bos>')\n",
    "        self.tokens.append('<eos>')\n",
    "        self.tokens.append('<pad>')\n",
    "        self.tokens.append('<unk>')\n",
    "\n",
    "        # add all the tokens \n",
    "        self.build_vocab()\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        all_tokens = [token for l_tokens in self.lil_tokens for token in l_tokens]\n",
    "        counter = Counter(all_tokens)\n",
    "        most_common = counter.most_common(self.max_vocab_size - len(self.tokens))\n",
    "        self.tokens += [item[0] for item in most_common]\n",
    "        self.ids = {token: id for id, token in enumerate(self.tokens)}\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "\n",
    "    def get_token(self, id):\n",
    "        return self.tokens[id]\n",
    "\n",
    "    def decode_idx2token(self, list_id):\n",
    "        return [self.tokens[i] for i in list_id]\n",
    "\n",
    "    def encode_token2idx(self, list_token):\n",
    "        return [self.ids[tok] if tok in self.ids else '<unk>' for tok in list_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        dataset = []\n",
    "        for line in f:\n",
    "            dataset.append(line.strip().split(' '))\n",
    "    return dataset\n",
    "\n",
    "def token2index_dataset(dataset_lil, vocab):\n",
    "    index_lil = []\n",
    "    for data in dataset_lil:\n",
    "        data = ['<bos>'] + data + ['<eos>']\n",
    "        index_data = vocab.encode_token2idx(data)\n",
    "        index_lil.append(index_data)\n",
    "\n",
    "    return index_lil\n",
    "\n",
    "def load_qa_data(file_path):\n",
    "    # read data \n",
    "    answer_lil = read_data(file_path['source'])\n",
    "    question_lil = read_data(file_path['target'])\n",
    "\n",
    "    # save list of words \n",
    "    main_df = pd.DataFrame()\n",
    "    main_df['source_data'] = answer_lil\n",
    "    main_df['target_data'] = question_lil\n",
    "\n",
    "    # build dictionary for source and target \n",
    "    answer_vocab = Vocabulary(answer_lil, 45_000)\n",
    "    question_vocab = Vocabulary(question_lil, 28_000)\n",
    "\n",
    "    # convert words to idx for each dataset\n",
    "    main_df['source_indized'] = token2index_dataset(answer_lil, answer_vocab)\n",
    "    main_df['target_indized'] = token2index_dataset(question_lil, question_vocab)\n",
    "\n",
    "    return main_df, answer_vocab, question_vocab\n",
    "\n",
    "\n",
    "class QAPair(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.main_df, self.answer_vocab, self.question_vocab = load_qa_data(file_path)\n",
    "        self.pad_idx = self.answer_vocab.get_id('<pad>')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.main_df.iloc[idx]['source_indized'], self.main_df.iloc[idx]['target_indized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch, pad_token):\n",
    "    # batch is a list of sample tuples\n",
    "    # the \"right\" length is the length of longest sentence\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "        \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = {\n",
    "    'source': \"data/processed/src-test.txt\",\n",
    "    'target': \"data/processed/tgt-test.txt\"\n",
    "}\n",
    "qa_test_dataset = QAPair(test_file_path)\n",
    "\n",
    "qa_test_loader = DataLoader(qa_test_dataset, batch_size=1024, shuffle=True, collate_fn=partial(pad_collate_fn, qa_test_dataset.pad_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d10e99a7d3ce7799ff53fffb01ded82fc9320fdeabd270dd8a29b265097acaf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
